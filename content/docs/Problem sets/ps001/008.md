# Exercise 08
{{< hint info >}}
Implement a pixelator video application and perform a benchmark of the results (color avg vs spatial coherence). How would you assess the visual quality of the results?
{{< /hint >}}

| Option                                                  | Key        |
|---------------------------------------------------------|------------|
| Play/Pause                                              | *P* or *p* |
| Mute/Unmute                                             | *M* or *m* |
| Decrease pixel size                                     | ,          |
| Increase pixel size                                     | .          |
| Toggle between original,  spatial coherence and average | *T* or *t* |

{{< p5-iframe sketch="/visualcomputing/sketches/ps001/ex08.js" ver="1.6.0" width="720" height="724" >}}

## Explanation

The standard method to benchmark noise is the signal to noise ratio (SNR), which measures the "noise" compared to the real signal. This is hard to do in the broader sense since we don't normally know how much noise has the original source.

A proposal for a benchmark of the pixelated video {{< katex >}}x{{< /katex >}} against the original video {{< katex >}}y{{< /katex >}} could be 
 

{{< katex display >}}
Benchmark(x)_y = sum(|Y_y - Y_x|)
{{< /katex >}}

or 

{{< katex display >}}
Benchmark(x)_y = avg(|Y_y - Y_x|)
{{< /katex >}}

Where the matrix {{< katex >}}Y_i{{< /katex >}} represents the Luma of video {{< katex >}}i{{< /katex >}} in the color space YCbCr. We do this because the luma represents the perception of the human eye better than the RGB plane.
Therefore an objective benchmarck is to get the absolute value of the difference between two lumas pixel by pixel, and then reduce this to one dimension by either summing every result or averaging, because every pixel counts.
